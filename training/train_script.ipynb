{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44c40343-0e1a-4e77-886b-2b3275db411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"TF_USE_NVLINK_FOR_PARALLEL_COMPILATION\"] = \"0\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "ENV = {\"TF_FORCE_UNIFIED_MEMORY\":\"1\", \"XLA_PYTHON_CLIENT_MEM_FRACTION\":\"4.0\"}\n",
    "for k,v in ENV.items():\n",
    "  os.environ[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d832127-791a-447c-a67a-b11010bfc67b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "DATASET = \"2k\"\n",
    "if DATASET == \"2k\":\n",
    "  with open(\"new_labels_2k.pkl\",\"rb\") as handle:\n",
    "    LABELS = pickle.load(handle)\n",
    "  with open(\"assignments_2k.pkl\",\"rb\") as handle:\n",
    "    CROSS_ASSIGNED = pickle.load(handle)\n",
    "  TMS = {}\n",
    "  for line in open(\"tmscores_2k.txt\",\"r\"):\n",
    "    a,b,tm_a,tm_b = line.rstrip().split()\n",
    "    tmscore = max(float(tm_a),float(tm_b))\n",
    "    if a not in TMS: TMS[a] = {}\n",
    "    if b not in TMS: TMS[b] = {}\n",
    "    TMS[a][b] = tmscore\n",
    "    TMS[b][a] = tmscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02143c78-bdf2-4299-b2a3-ec9460fcfa30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def get_data(M=\"split\",\n",
    "             feats=\"pair_A\",\n",
    "             pad_len=500,\n",
    "             mask_alt=False,\n",
    "             seed=0):\n",
    "        \n",
    "  data = [[],[],[]]\n",
    "  labels = [[],[],[]]\n",
    "  sample_weight = [[],[],[]]\n",
    "  info = [[],[],[]]\n",
    "  \n",
    "  dist_bins = np.append(np.linspace(3.2,6.0,15),np.inf)\n",
    "  for a,v in CROSS_ASSIGNED[seed].items():\n",
    "    info[v].append(a)\n",
    "    \n",
    "    # get labels\n",
    "    y = LABELS[a]\n",
    "    L = y[\"dist_all\"].shape[0]\n",
    "    if L <= 500:\n",
    "      y_dist = np.sum(dist_bins < y[\"dist_all\"][...,None],-1)\n",
    "\n",
    "      # all ligands\n",
    "      y_bind_all = y[\"dist_all\"] < 5.0\n",
    "\n",
    "      # confident ligands\n",
    "      y_bind_sub = y[\"dist_sub\"] < 5.0\n",
    "\n",
    "      # homologous ligands (with TMscore > 0.8)\n",
    "      y_bind_alt = y[\"bind_alt\"]\n",
    "\n",
    "      # adjust mask\n",
    "      y_mask = y[\"mask\"].copy()\n",
    "      y_mask[np.logical_and(y_bind_all == True, y_bind_sub == False)] = False\n",
    "\n",
    "      if mask_alt:\n",
    "        # mask alternative binding positions\n",
    "        y_mask[np.logical_and(y_bind_all == False, y_bind_alt == True)] = False  \n",
    "\n",
    "      y = np.stack([y_bind_all, y_mask, y_dist],-1)\n",
    "\n",
    "      # gather data\n",
    "      if M == \"esm2\":\n",
    "        x = np.load(f\"embeddings/esm2/{a}.npz\")\n",
    "        if feats == \"last\":\n",
    "          x = x[\"rep\"][x[\"idx\"],-1]\n",
    "        else:\n",
    "          x = x[\"rep\"][x[\"idx\"]]\n",
    "      elif feats == \"pair_B\":\n",
    "        x = np.load(f\"embeddings/{M}/{a}.npz\")[\"pair_B\"].swapaxes(0,1)\n",
    "      else:\n",
    "        x = np.load(f\"embeddings/{M}/{a}.npz\")[feats]\n",
    "\n",
    "      # reshape\n",
    "      x = x.reshape(x.shape[0],-1)\n",
    "\n",
    "      # pad\n",
    "      l = x.shape[0]\n",
    "      l_ = y.shape[0]\n",
    "      if l != l_: print(a)\n",
    "      data[v].append(np.pad(x,[[0,pad_len-l],[0,0]]))\n",
    "      labels[v].append(np.pad(y,[[0,pad_len-l],[0,0]]))    \n",
    "\n",
    "      # weight\n",
    "      counts = 1\n",
    "      for b,_ in CROSS_ASSIGNED[seed].items():\n",
    "        if a != b:\n",
    "          if TMS[a][b] > 0.5: counts += 1\n",
    "\n",
    "      sample_weight[v].append(1/counts)\n",
    "    else:\n",
    "      print(a,L)\n",
    "  \n",
    "  # combine\n",
    "  data = [np.array(v) for v in data]\n",
    "  labels = [np.array(v) for v in labels]\n",
    "  sample_weight = [np.array(v) for v in sample_weight]\n",
    "    \n",
    "  return data, labels, sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "795a8dc5-7085-4681-9d8f-4a9079e25916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "import optax\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from jax import tree_util\n",
    "\n",
    "def l2_regularization(params, lmbda=1.0):\n",
    "    \"\"\"Compute L2 regularization for a nested dictionary of parameters.\"\"\"\n",
    "    \n",
    "    # Function to square and sum each array\n",
    "    squared_and_summed = tree_util.tree_map(lambda x: jnp.sum(jnp.square(x)), params)\n",
    "    \n",
    "    # Summing all the individual sums\n",
    "    total_l2_penalty = sum(tree_util.tree_flatten(squared_and_summed)[0])\n",
    "    \n",
    "    return lmbda * total_l2_penalty\n",
    "\n",
    "\n",
    "class mk_model:\n",
    "  def __init__(self, data, labels, sample_weight, normalize=True, lam=0.01):\n",
    "      \n",
    "      self.data = data\n",
    "      self.labels = labels\n",
    "      self.sample_weight = sample_weight\n",
    "      self.normalize = normalize\n",
    "      self.history = []\n",
    "      self.lam = lam\n",
    "\n",
    "      # Normalize data\n",
    "      if normalize:\n",
    "        mask = sample_weight[0][:,None,None] * labels[0][...,1][:,:,None]\n",
    "        self._mean = (data[0] * mask).sum((0,1)) / mask.sum((0,1))\n",
    "        self._std = np.sqrt((np.square(data[0] - self._mean) * mask).sum((0,1)) / mask.sum((0,1)))\n",
    "\n",
    "      self.F = data[0].shape[-1]\n",
    "      \n",
    "      # copy data to GPU\n",
    "      self._data = [jnp.array(x) for x in self.data]\n",
    "      self._labels = [jnp.array(x) for x in self.labels]\n",
    "      self._sample_weight = [jnp.array(x) for x in self.sample_weight]\n",
    "\n",
    "      \n",
    "      # Define the model function and initialize it\n",
    "      self.model = hk.without_apply_rng(hk.transform(self._build_model))\n",
    "      rng = jax.random.PRNGKey(42)\n",
    "      self.params = self.model.init(rng, data[0])\n",
    "\n",
    "      # Define optimizer\n",
    "      self.recompile(1e-3)\n",
    "\n",
    "  def _build_model(self, x):\n",
    "    if self.normalize:\n",
    "      mean = hk.get_parameter(\"mean\", shape=(self.F,), init=hk.initializers.Constant(self._mean))\n",
    "      std = hk.get_parameter(\"std\", shape=(self.F,), init=hk.initializers.Constant(self._std))\n",
    "      x = (x - jax.lax.stop_gradient(mean)) / jax.lax.stop_gradient(std)\n",
    "        \n",
    "    # final layer\n",
    "    x = jax.nn.sigmoid(hk.Linear(1, w_init=hk.initializers.Constant(0))(x))\n",
    "    return x\n",
    "\n",
    "  def loss_fn(self, params, inputs, targets, sample_weights):\n",
    "      \"\"\"Custom loss function.\"\"\"\n",
    "      predictions = self.model.apply(params, inputs)\n",
    "      y_true_bce = targets[..., 0]\n",
    "      y_mask = targets[..., 1]\n",
    "      loss = -y_true_bce * jnp.log(predictions[..., 0] + 1e-7) - (1 - y_true_bce) * jnp.log(1 - predictions[..., 0] + 1e-7)\n",
    "      masked_loss = jnp.sum(loss * y_mask, axis=-1) / (jnp.sum(y_mask, axis=-1) + 1e-7)\n",
    "      weighted_loss = (masked_loss * sample_weights).sum() / (sample_weights.sum() + 1e-7)\n",
    "\n",
    "      \n",
    "      # l2\n",
    "      l2_loss = []\n",
    "      for _,p in params.items():\n",
    "        if \"w\" in p:\n",
    "          l2_loss.append(jnp.square(p[\"w\"]).sum())\n",
    "      # Combine all losses\n",
    "      total_loss = weighted_loss + self.lam * sum(l2_loss)\n",
    "\n",
    "      return total_loss\n",
    "\n",
    "  def fit(self, epochs=1, batch_size=64, verbose=True):\n",
    "      num_samples = len(self._data[0])\n",
    "      indices = np.arange(num_samples)\n",
    "      for epoch in range(epochs):\n",
    "          # Shuffle indices each epoch\n",
    "          np.random.shuffle(indices)\n",
    "          running_loss = 0\n",
    "          steps_per_epoch = num_samples // batch_size + int(num_samples % batch_size != 0)\n",
    "          pbar = tqdm(total=steps_per_epoch, disable=not verbose, dynamic_ncols=True)\n",
    "          for i in range(0, num_samples, batch_size):\n",
    "              batch_indices = indices[i:i + batch_size]\n",
    "              batch_data = self._data[0][batch_indices]\n",
    "              batch_labels = self._labels[0][batch_indices]\n",
    "              batch_sample_weights = self._sample_weight[0][batch_indices]\n",
    "              self.params, self.opt_state, loss = self.update(self.params, self.opt_state, batch_data, batch_labels, batch_sample_weights)\n",
    "              running_loss += loss\n",
    "              avg_loss = running_loss / (i // batch_size + 1)\n",
    "              pbar.set_description(f\"loss: {avg_loss:.4f}\")\n",
    "              pbar.update(1)\n",
    "          pbar.close()\n",
    "\n",
    "  def predict(self, mode=0, batch_size=12, verbose=True):\n",
    "    data = self.data[mode]\n",
    "    num_samples = len(data)\n",
    "    indices = np.arange(num_samples)    \n",
    "    predictions = []    \n",
    "    pbar = tqdm(total=num_samples, disable=not verbose, desc=\"Predicting\", dynamic_ncols=True)    \n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        batch_indices = indices[i:i+batch_size]\n",
    "        batch_data = data[batch_indices]\n",
    "        batch_predictions = self.model.apply(self.params, batch_data)        \n",
    "        predictions.append(batch_predictions)        \n",
    "        pbar.update(len(batch_data))        \n",
    "    pbar.close()    \n",
    "    return np.concatenate(predictions, axis=0)\n",
    "  \n",
    "  def recompile(self, learning_rate=None):\n",
    "    if learning_rate is not None:\n",
    "      self.optimizer = optax.adam(learning_rate)\n",
    "      self.opt_state = self.optimizer.init(self.params)\n",
    "      def _update(params, opt_state, inputs, targets, sample_weights):\n",
    "        loss_val, grads = jax.value_and_grad(self.loss_fn)(params, inputs, targets, sample_weights)\n",
    "        updates, opt_state = self.optimizer.update(grads, opt_state)\n",
    "        new_params = optax.apply_updates(params, updates)\n",
    "        return new_params, opt_state, loss_val\n",
    "      self.update = jax.jit(_update)\n",
    "\n",
    "  def recovery(self, mode=0, mean=True, batch_size=12, k=None,):\n",
    "    # Predict\n",
    "    predictions = self.predict(mode, batch_size=batch_size, verbose=False)\n",
    "    predictions = predictions[...,0]\n",
    "\n",
    "    true_labels = self.labels[mode][...,0]\n",
    "    mask = self.labels[mode][...,1]\n",
    "    weights = self.sample_weight[mode]\n",
    "\n",
    "    t_rec = []\n",
    "    p_rec = []\n",
    "\n",
    "    for n in range(len(true_labels)):\n",
    "      mask_ = mask[n] == True\n",
    "      pred_ = predictions[n][mask_]\n",
    "      true_ = true_labels[n][mask_]\n",
    "\n",
    "      top_k = sum(true_) if k is None else k\n",
    "      sorted_indices = pred_.argsort()[::-1][:top_k]\n",
    "      top_k_pred = pred_[sorted_indices]\n",
    "      top_k_true = true_[sorted_indices]\n",
    "\n",
    "      t_rec.append(top_k_true.mean())\n",
    "      p_rec.append(top_k_pred.mean())\n",
    "\n",
    "    if mean:\n",
    "      t_rec = (np.array(t_rec) * weights).sum() / weights.sum()\n",
    "      p_rec = (np.array(p_rec) * weights).sum() / weights.sum()\n",
    "\n",
    "    return [p_rec,t_rec]\n",
    "\n",
    "  def evaluate(self):\n",
    "      self.history.append([self.recovery(0), self.recovery(1), self.recovery(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb3dd102-82a9-4cef-8644-17ec3159b15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_mem():\n",
    "  backend = jax.lib.xla_bridge.get_backend()\n",
    "  for buf in backend.live_buffers(): buf.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "854e5daa-797a-44bc-9a2c-8710a0a3f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ff91e6-840e-47ac-940d-a92895ad904a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODE = f\"attempt_7_{DATASET}\"\n",
    "LAM = 0.03\n",
    "lam_str = str(LAM).replace(\".\",\"-\")\n",
    "NAME = f\"{MODE}_lam{lam_str}\"\n",
    "os.makedirs(f\"fix/{NAME}\",exist_ok=True)\n",
    "NORMALIZE_BY_TRAIN = True\n",
    "MASK_ALT = False\n",
    "REDO = False\n",
    "H10 = {}\n",
    "for X in [\n",
    "  [[\"split_nosc\",\"pair_A\"],[\"split_nosc\",\"pair_B\"]],\n",
    "]:\n",
    "  for seed in range(10):\n",
    "    clear_mem()\n",
    "    name = \"_\".join(sum(X,[]))+f\"_{seed}\"\n",
    "    print(name)    \n",
    "    if REDO or not os.path.isfile(f\"fix/{NAME}/\"+name+\".history.npy\"):\n",
    "      data, labels, sample_weight = get_data(*X[0], seed=seed, mask_alt=MASK_ALT)\n",
    "      print(X[0])\n",
    "      if len(X) > 1:\n",
    "        for x in X[1:]:\n",
    "          print(x)\n",
    "          data_,_,_ = get_data(*x,\n",
    "                               seed=seed,\n",
    "                               mask_alt=MASK_ALT) \n",
    "          for k,v in enumerate(data):\n",
    "            data[k] = np.concatenate([v,data_[k]],-1)\n",
    "\n",
    "      model = mk_model(\n",
    "              labels = labels,\n",
    "              data = data,\n",
    "              sample_weight=sample_weight,\n",
    "              normalize=NORMALIZE_BY_TRAIN,\n",
    "              lam=LAM,\n",
    "            )\n",
    "      model.recompile(learning_rate=1e-3)\n",
    "      for _ in range(40):\n",
    "        model.fit(epochs=4,  batch_size=12, verbose=False)\n",
    "        model.evaluate()\n",
    "        print(model.history[-1])\n",
    "      model.recompile(learning_rate=1e-4)\n",
    "      for _ in range(40):\n",
    "        model.fit(epochs=4,  batch_size=12, verbose=False)\n",
    "        model.evaluate()\n",
    "        print(model.history[-1])\n",
    "      h = np.array(model.history)\n",
    "      filename = f\"fix/{NAME}/\"+name\n",
    "      with open(filename+\".pickle\",\"wb\") as handle:\n",
    "        pickle.dump(model.params, handle)\n",
    "      np.save(filename+\".history.npy\", np.array(model.history))\n",
    "      del data\n",
    "      gc.collect()\n",
    "    else:\n",
    "      h = np.load(f\"fix/{NAME}/\"+name+\".history.npy\")\n",
    "    H10[name] = h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
